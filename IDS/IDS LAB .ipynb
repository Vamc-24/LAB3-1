{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21948336",
   "metadata": {},
   "source": [
    "Program 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([10,15,20,25])\n",
    "print(\"Basic array:\")\n",
    "print(a)\n",
    "b=np.zeros((3,3),dtype=int)\n",
    "print(\"Array of zeros:\")\n",
    "print(b)\n",
    "c=np.ones((3,3),dtype=int)\n",
    "print(\"Array of ones:\")\n",
    "print(c)\n",
    "r=np.random.random((3,3))\n",
    "print(\"Random numbers in nd array:\")\n",
    "print(r)\n",
    "d=np.array([[10,20,30],[40,50,60],[60,70,90]])\n",
    "print(\"Choice array:\")\n",
    "print(d)\n",
    "i=np.eye(3,dtype=int)\n",
    "print(\"Imatrix in numpy:\")\n",
    "print(i)\n",
    "e=np.arange(0,10,2)\n",
    "print(\"Evenly spaced array:\")\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b693a14a",
   "metadata": {},
   "source": [
    "Program 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([[20,40,50,60],\n",
    "            [80,90,90,94]])\n",
    "print(\"Dimensions of numpy array:\")\n",
    "print(a.ndim)\n",
    "print(\"Shape of numpy array:\")\n",
    "print(a.shape)\n",
    "print(a.size)\n",
    "\n",
    "print(\"Reshaping a numpy  array:\\n\",a.reshape(4,2))\n",
    "c=a.flatten()\n",
    "print(\"Flattening a numpy array:\")\n",
    "print(c)\n",
    "d=a.transpose()\n",
    "print(\"Transpose of a numpy array:\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245bb342",
   "metadata": {},
   "source": [
    "Program 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401781dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array: [100 200 300]\n",
      "Expanded array(axis=0):\n",
      " [[100 200 300]]\n",
      "Expanded array(axis=1):\n",
      " [[100]\n",
      " [200]\n",
      " [300]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array=np.array([100,200,300])\n",
    "print(\"Array:\",array)\n",
    "expandarray=np.expand_dims(array,axis=0)\n",
    "print(\"Expanded array(axis=0):\\n\",expandarray)\n",
    "expandarray1=np.expand_dims(array,axis=1)\n",
    "print(\"Expanded array(axis=1):\\n\",expandarray1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db2c5d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (1, 3, 1, 4)\n",
      "Squeezedshape: (3, 4)\n",
      "[[ 10  20  30  40]\n",
      " [ 50  60  70  80]\n",
      " [ 90 100 110 120]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array=np.array([[[[10,20,30,40]],\n",
    "                [[50,60,70,80]],\n",
    "                [[90,100,110,120]]]])\n",
    "print(\"Original shape:\",array.shape)\n",
    "squeezedarray=np.squeeze(array)\n",
    "print(\"Squeezedshape:\",squeezedarray.shape)\n",
    "print(squeezedarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30bcbae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted along axis0(columns):\n",
      " [[30 10 20]\n",
      " [50 40 60]]\n",
      "Sorted along axis 1(rows):\n",
      " [[10 20 30]\n",
      " [40 50 60]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array=np.array([[30,10,20],[50,40,60]])\n",
    "sortedarray=np.sort(array,axis=0)\n",
    "print(\"Sorted along axis0(columns):\\n\",sortedarray)\n",
    "sortedarray=np.sort(array,axis=1)\n",
    "print(\"Sorted along axis 1(rows):\\n\",sortedarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4793ef",
   "metadata": {},
   "source": [
    "Program 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9346ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array1d[2:7]: [30 40 50 60 70]\n",
      "array1d[:5]: [10 20 30 40 50]\n",
      "array1d[5:]: [ 60  70  80  90 100]\n",
      "array1d[::2]: [10 30 50 70 90]\n",
      "array1d[1::2]: [ 20  40  60  80 100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array1d=np.array([10,20,30,40,50,60,70,80,90,100])\n",
    "print(\"array1d[2:7]:\",array1d[2:7])\n",
    "print(\"array1d[:5]:\",array1d[:5])\n",
    "print(\"array1d[5:]:\",array1d[5:])\n",
    "print(\"array1d[::2]:\",array1d[::2])\n",
    "print(\"array1d[1::2]:\",array1d[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae14b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array2d[1:3,1:3]:\n",
      " [[30 35]\n",
      " [50 55]]\n",
      "array2d[:2,:2]:\n",
      " [[ 5 10]\n",
      " [25 30]]\n",
      "array2d[2:,2:]:\n",
      " [[55 60]\n",
      " [75 80]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array2d=np.array([[5,10,15,20],\n",
    "                  [25,30,35,40],\n",
    "                  [45,50,55,60],\n",
    "                  [65,70,75,80]])\n",
    "print('array2d[1:3,1:3]:\\n',array2d[1:3,1:3])\n",
    "print('array2d[:2,:2]:\\n',array2d[:2,:2])\n",
    "print('array2d[2:,2:]:\\n',array2d[2:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "676381d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array3d[0:2,1:3,1:3]:\n",
      " [[[10 12]\n",
      "  [16 18]]\n",
      "\n",
      " [[28 30]\n",
      "  [34 36]]]\n",
      "array3d[:,1,:]:\n",
      " [[ 8 10 12]\n",
      " [26 28 30]\n",
      " [44 46 48]]\n",
      "array3d[:,:,1:3]:\n",
      " [[[ 4  6]\n",
      "  [10 12]\n",
      "  [16 18]]\n",
      "\n",
      " [[22 24]\n",
      "  [28 30]\n",
      "  [34 36]]\n",
      "\n",
      " [[40 42]\n",
      "  [46 48]\n",
      "  [52 54]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array3d=np.array([\n",
    "                   [ [2,4,6],\n",
    "                     [8,10,12],\n",
    "                     [14,16,18]],\n",
    "\n",
    "                   [[20,22,24],\n",
    "                    [26,28,30],\n",
    "                    [32,34,36]], \n",
    "\n",
    "                    [[38,40,42],\n",
    "                     [44,46,48],\n",
    "                     [50,52,54]] \n",
    "                     ] )\n",
    "print('array3d[0:2,1:3,1:3]:\\n',array3d[0:2,1:3,1:3])\n",
    "print('array3d[:,1,:]:\\n',array3d[:,1,:])\n",
    "print('array3d[:,:,1:3]:\\n',array3d[:,:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfe814c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array1d[-5:]: [ 60  70  80  90 100]\n",
      "array1d[:-5]: [10 20 30 40 50]\n",
      "array2d[-2:-2:]:\n",
      " [[55 60]\n",
      " [75 80]]\n",
      "array3d[:,-2:,-2:]: [[[10 12]\n",
      "  [16 18]]\n",
      "\n",
      " [[28 30]\n",
      "  [34 36]]\n",
      "\n",
      " [[46 48]\n",
      "  [52 54]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"array1d[-5:]:\",array1d[-5:])\n",
    "print(\"array1d[:-5]:\",array1d[:-5])\n",
    "print('array2d[-2:-2:]:\\n',array2d[-2:,-2:])\n",
    "print(\"array3d[:,-2:,-2:]:\",array3d[:,-2:,-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727e5d8",
   "metadata": {},
   "source": [
    "Program 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b25298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack arrays along new axis:\n",
      "[[10 20 30]\n",
      " [40 50 60]]\n",
      "Horizontal stack:\n",
      "[10 20 30 40 50 60]\n",
      "Vertical stack:\n",
      "[[10 20 30]\n",
      " [40 50 60]]\n",
      "Depth stack:\n",
      "[[[10 40]\n",
      "  [20 50]\n",
      "  [30 60]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a1=np.array([10,20,30])\n",
    "a2=np.array([40,50,60])\n",
    "stacked=np.stack((a1,a2),axis=0)\n",
    "print(\"Stack arrays along new axis:\")\n",
    "print(stacked)\n",
    "hstacked=np.hstack((a1,a2))\n",
    "print(\"Horizontal stack:\")\n",
    "print(hstacked)\n",
    "vstacked=np.vstack((a1,a2))\n",
    "print(\"Vertical stack:\")\n",
    "print(vstacked)\n",
    "dstacked=np.dstack((a1,a2))\n",
    "print(\"Depth stack:\")\n",
    "print(dstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260960a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a1=np.array([[10,20],[30,40]])\n",
    "a2=np.array([[50,60],[70,80]])\n",
    "concataxis0=np.concatenate((a1,a2),axis=0)\n",
    "print(\"Concatenate along axis0(columns):\")\n",
    "print(concataxis0)\n",
    "concataxis1=np.concatenate((a1,a2),axis=1)\n",
    "print(\"Concatenate along axis1(rows):\")\n",
    "print(concataxis1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a1=np.array([10,20,30])\n",
    "a2=np.array([[40],[50],[60]])\n",
    "result=a1+a2\n",
    "print(\"Broadcasting addition:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b4072",
   "metadata": {},
   "source": [
    "Program 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data1={\n",
    "    'Name':['Virat kohli','Yuvraj singh','MS Dhoni'],\n",
    "    'Age':[40,39,40],\n",
    "    'City':['Ranchi','punjab','Delhi']\n",
    "}\n",
    "df=pd.DataFrame(data1)\n",
    "data2={\n",
    "    'Name':['Rohith sharma','Sachin'],\n",
    "    'Age':[40,39],\n",
    "    'City':['Mumbai','Mumbai']\n",
    "}\n",
    "df2=pd.DataFrame(data2)\n",
    "\n",
    "dfconcat=pd.concat([df,df2],ignore_index=True)\n",
    "\n",
    "dffiltered=dfconcat[dfconcat['Age']>35]\n",
    "\n",
    "dfconcat['senior']=dfconcat['Age']>=40\n",
    "\n",
    "print(\"Original dataframe:\")\n",
    "print(df)\n",
    "print(\"\\nConcatenated dataframe:\")\n",
    "print(dfconcat)\n",
    "print(\"\\nfiltered dataframe(Age>35):\")  \n",
    "print(dffiltered)\n",
    "print(\"\\nDataframe with new 'senior' column:\")\n",
    "print(dfconcat) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3363d1f",
   "metadata": {},
   "source": [
    "Program 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306b8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data1={\n",
    "    'Employee':['A','B','C','D','E'],\n",
    "    'Department':['HR','Finance','IT',np.nan,'IT'],\n",
    "    'Salary':[50000,60000,np.nan,80000,45000]\n",
    "    }\n",
    "df=pd.DataFrame(data1)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "dffilled=df.fillna({'Department':'Unknown','Salary':0})\n",
    "print(\"\\nDataFrame with NaN filled:\")\n",
    "print(dffilled)\n",
    "dfsorted=dffilled.sort_values(by='Salary',ascending=False)\n",
    "print(\"\\nDataFrame sorted by 'Salary'(Descending):\\n\",dfsorted)\n",
    "dfsorted['Salary']=dfsorted['Salary'].replace(0,np.nan)\n",
    "dfgrouped=dfsorted.groupby('Department')['Salary'].mean().reset_index()\n",
    "print(\"Mean salary for each department:\\n\",dfgrouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be569d2a",
   "metadata": {},
   "source": [
    "Program 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128fd8b",
   "metadata": {},
   "source": [
    "Reading text files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14848248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"textfile.txt\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab262c",
   "metadata": {},
   "source": [
    "Reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfcsv=pd.read_csv(\"fifa_data.csv\")\n",
    "dfcsv.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7299437",
   "metadata": {},
   "source": [
    "Reading Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1feb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_excel(\"capbudg.xls\")\n",
    "df=pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d64c4",
   "metadata": {},
   "source": [
    "Reading JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data={\n",
    "    \"NAme\":\"A\",\n",
    "    \"Age\":40,\n",
    "    \"City\":\"Tirupati\",\n",
    "    \"Haschildren\":False,\n",
    "    \"Hobbies\":[\"Reading\",\"Travelling\",\"Swimming\"]\n",
    "}\n",
    "filename=\"data.json\"\n",
    "with open(filename,'w')as json_file:\n",
    "    json.dump(data,json_file,indent=4)\n",
    "print(\"JSON file {} created successfully\".format(filename))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjson=pd.read_json(\"data.json\")\n",
    "print(dfjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e47c4b",
   "metadata": {},
   "source": [
    "Program 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0b461",
   "metadata": {},
   "source": [
    "Reading pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9855353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class Emp:\n",
    "    def __init__(self,eno,ename,esal,eaddr):\n",
    "        self.eno=eno\n",
    "        self.ename=ename\n",
    "        self.esal=esal\n",
    "        self.eaddr=eaddr\n",
    "\n",
    "    def display(self):\n",
    "        print(\"eno:{},ename:{},esal:{},eaddr:{}\".format(self.eno,self.ename,self.esal,self.eaddr))\n",
    "\n",
    "e= Emp(10,\"Alice\",1000,\"TPT\")\n",
    "\n",
    "with open(\"emp.txt\",'wb') as f:\n",
    "        pickle.dump(e,f)\n",
    "print(\"Pickling of employee is completed\")\n",
    "with open (\"emp.txt\",'rb') as f:\n",
    "    obj=pickle.load(f)\n",
    "print(\"Unpickling of employee is completed\")\n",
    "obj.display()    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e0a18",
   "metadata": {},
   "source": [
    "Reading image files using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58429a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "from IPython.display import display\n",
    "image=Image.open(\"ds.jpeg\")\n",
    "display(image)\n",
    "print(\"Image format:\",image.format)\n",
    "print(\"Image size:\",image.size)\n",
    "print(\"Image mode:\",image.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12dfcd",
   "metadata": {},
   "source": [
    "Reading multiple files using Glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files=glob.glob(\"*.txt\")\n",
    "print(\"List of text files:\",files)\n",
    "for file in files:\n",
    "    with  open (file,\"r\",encoding='utf-8') as f:\n",
    "       print(f\"Contents of {file}:\")\n",
    "       print(f.read())\n",
    "      \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf969949",
   "metadata": {},
   "source": [
    "Importing data from a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "db_path=\"test.db\"\n",
    "conn=sqlite3.connect(db_path)\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS Students(id integer,name text,age integer)\")\n",
    "cursor.execute(\"Insert into students values(1,'Alice',22)\")\n",
    "cursor.execute(\"insert into students values(2,'Bob',23)\")\n",
    "conn.commit()\n",
    "print(\"Data inserted successfullly\")\n",
    "cursor.execute(\"select*from students\")\n",
    "rows=cursor.fetchall()\n",
    "print(\"Database data:\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "conn.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c138e17",
   "metadata": {},
   "source": [
    "Program 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://quotes.toscrape.com/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "quotes=soup.find_all('div',class_='quote')\n",
    "for quote in quotes:\n",
    "    text=quote.find('span',class_='text').get_text()\n",
    "    author=quote.find('small',class_='author').get_text()\n",
    "    tags=[tag.get_text() for tag in quote.find_all('a',class_='tag')]         \n",
    "    print(f\"Quote:{text}\")     \n",
    "    print(f\"Author:{author}\")          \n",
    "    print(f\"Tags:{','.join(tags)}\\n\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208a16e",
   "metadata": {},
   "source": [
    "Program 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "data = {\n",
    "    'Loan_ID': ['LP001002', 'LP001003', 'LP001005', 'LP001006', 'LP001008', 'LP001011', 'LP001013'],\n",
    "    'Gender': ['Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female'],\n",
    "    'Married': ['Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No'],\n",
    "    'Dependents': [0, 1, 0, 0, 2, 0, 0],\n",
    "    'Education': ['Graduate', 'Graduate', 'Graduate', 'Not Graduate', 'Graduate', 'Graduate', 'Graduate'],\n",
    "    'ApplicantIncome': [5849, 4583, 3000, 2583, 6000, 5417, 2333],\n",
    "    'CoapplicantIncome': [0.0, 1508.0, 0.0, 2358.0, 0.0, 4196.0, 1516.0],\n",
    "    'LoanAmount': [128, 128, 66, 120, 141, 267, 95],\n",
    "    'Loan_Amount_Term': [360, 360, 360, 360, 360, 360, 360],\n",
    "    'Credit_History': [1, 1, 1, 1, 1, 1, 1],\n",
    "    'Property_Area': ['Urban', 'Rural', 'Urban', 'Urban', 'Urban', 'Urban', 'Rural']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MinMaxScaler()\n",
    "dfscaled = df.copy()\n",
    "\n",
    "dfscaled[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']] = scaler.fit_transform(\n",
    "    dfscaled[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\n",
    ")\n",
    "print(\"\\nData after Feature Scaling:\")\n",
    "print(dfscaled)\n",
    "\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "dfstandardized = df.copy()\n",
    "dfstandardized[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']] = std_scaler.fit_transform(\n",
    "    dfstandardized[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\n",
    ")\n",
    "print(\"\\nData after Feature Standardization:\")\n",
    "print(dfstandardized)\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "dflabelencoded = df.copy()\n",
    "dflabelencoded['Gender'] = labelencoder.fit_transform(dflabelencoded['Gender'])\n",
    "dflabelencoded['Married'] = labelencoder.fit_transform(dflabelencoded['Married'])\n",
    "dflabelencoded['Education'] = labelencoder.fit_transform(dflabelencoded['Education'])\n",
    "dflabelencoded['Property_Area'] = labelencoder.fit_transform(dflabelencoded['Property_Area'])\n",
    "print(\"\\nData after Label Encoding:\")\n",
    "print(dflabelencoded)\n",
    "\n",
    "\n",
    "dfonehotencoded = pd.get_dummies(df, columns=['Gender', 'Married', 'Education', 'Property_Area'])\n",
    "print(\"\\nData after One Hot Encoding:\")\n",
    "print(dfonehotencoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e0154",
   "metadata": {},
   "source": [
    "Program 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170152a1",
   "metadata": {},
   "source": [
    "Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeab5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f493ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "students=['A','B','C','D']\n",
    "marks=[95,90,93,94]\n",
    "plot.bar(students,marks,color='green')\n",
    "plot.title(\"Bar Graph Marks\")\n",
    "plot.xlabel(\"categories\")\n",
    "plot.ylabel(\"Values\")\n",
    "plot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729e61e",
   "metadata": {},
   "source": [
    "pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f931087",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes=[25,25,30,20]\n",
    "labels=['A','B','C','D']\n",
    "plot.pie(sizes,labels=labels,autopct='%.2f%%',startangle=140)\n",
    "plot.title(\"pie Chart Example\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48efcd1",
   "metadata": {},
   "source": [
    "Box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf93446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple box plot\n",
    "import numpy as np\n",
    "data=[np.arange(0,50,5)]\n",
    "plot.boxplot(data,patch_artist=True,tick_labels=['X1'])\n",
    "plot.title(\"Box plot Example\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92554272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=[np.random.normal(0,std,100)for std in range(1,4)]\n",
    "plot.boxplot(data,vert=True,patch_artist=True,tick_labels=['X1','X2','X3'])\n",
    "plot.title(\"Box plot Example\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268e9bd",
   "metadata": {},
   "source": [
    "Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.random.randn(1000)\n",
    "#plot.hist(data,bins=30,color='green',alpha=0.7)\n",
    "plot.hist(data,bins=30,color='b')\n",
    "plot.title('Histogram Example')\n",
    "plot.xlabel('Value')\n",
    "plot.ylabel('Frequency')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d61b7a",
   "metadata": {},
   "source": [
    "Line chart and subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(0,10,100)\n",
    "y1=np.sin(x)\n",
    "y2=np.cos(x)\n",
    "fig,axs=plot.subplots(2)\n",
    "axs[0].plot(x,y1,label='sin(x)')\n",
    "axs[0].set_title('Sine Wave')\n",
    "axs[0].legend()\n",
    "axs[1].plot(x,y2,label='cos(x)')\n",
    "axs[1].set_title('Cosine Wave')\n",
    "axs[1].legend()\n",
    "plot.tight_layout()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef179e",
   "metadata": {},
   "source": [
    "Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bdd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=np.random.rand(50)\n",
    "y=np.random.rand(50)\n",
    "plot.scatter(x,y,color='red')\n",
    "plot.title('Scatter plot Example')\n",
    "plot.xlabel('x-axis')\n",
    "plot.ylabel('y-axis')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979be9e",
   "metadata": {},
   "source": [
    "Program 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "text=\"Hello! How are you doing today?\"\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f32eb1",
   "metadata": {},
   "source": [
    "Program 13 shortened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d85805",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43673291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Hello! How are you doing today?\"\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92307489",
   "metadata": {},
   "source": [
    "Program 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce1f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn import metrics \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd \n",
    " \n",
    "# Download required NLTK resources \n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    " \n",
    "# Sample text data \n",
    "data = { \n",
    "    'text': [ \n",
    "        'I love programming in Python!', \n",
    "        'Python is an amazing language.', \n",
    "        'I hate getting errors in my code.', \n",
    "        'Machine learning is fascinating.', \n",
    "        'Natural Language Processing is part of AI.', \n",
    "        'I enjoy solving problems with data.', \n",
    "        'My code is working perfectly.', \n",
    "        'Data science is the future of technology.'  \n",
    " \n",
    "    ], \n",
    "    'label': ['positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive'] \n",
    "} \n",
    " \n",
    "# Create a DataFrame \n",
    "df = pd.DataFrame(data) \n",
    " \n",
    "# Preprocessing: Tokenize and remove stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    " \n",
    "def preprocess_text(text): \n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words] \n",
    "    return ' '.join(filtered_tokens) \n",
    " \n",
    "df['text'] = df['text'].apply(preprocess_text) \n",
    " \n",
    "# Convert text data to numerical features \n",
    "vectorizer = TfidfVectorizer() \n",
    "X = vectorizer.fit_transform(df['text']) \n",
    " \n",
    "# Define target variable \n",
    "y = df['label'] \n",
    " \n",
    "# Split the dataset into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    " \n",
    "# Train a classifier \n",
    "model = MultinomialNB() \n",
    "model.fit(X_train, y_train) \n",
    " \n",
    "# Make predictions \n",
    "y_pred = model.predict(X_test) \n",
    " \n",
    "# Evaluate the model \n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) \n",
    "print(f'Accuracy: {accuracy:.2f}') \n",
    " \n",
    "# Display classification report \n",
    "print(metrics.classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22458575",
   "metadata": {},
   "source": [
    "Program 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056d204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\saiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa27def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\saiva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'in', 'Python', '.']\n",
      "POS Tags: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# Sample text\n",
    "text = \"NLTK is a powerful library for natural language processing in Python.\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "# Part-of-Speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Display the result\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
